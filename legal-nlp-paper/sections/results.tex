\section{Results and Discussion}

\subsection{Model Performance on CUAD Dataset}

The fine-tuned Legal-BERT model demonstrates strong performance on the multi-label clause extraction task, achieving competitive results despite the severe class imbalance inherent in the CUAD dataset. Table \ref{tab:model_performance} summarizes the key performance metrics.

\begin{table}[htbp]
\centering
\caption{Model Performance on CUAD Test Set}
\label{tab:model_performance}
\begin{tabular}{@{}lc@{}}
\toprule
Metric & Score \\
\midrule
F1-Score (Micro) & 0.880 \\
F1-Score (Macro) & 0.860 \\
F1-Score (Weighted) & 0.877 \\
Precision (Micro) & 0.875 \\
Recall (Micro) & 0.885 \\
Hamming Loss & 0.120 \\
Jaccard Similarity & 0.800 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Explainability Analysis Results}

The comprehensive explainability analysis reveals several key insights into model behavior and decision-making patterns. The confidence analysis demonstrates effective separation between true positive and true negative predictions, with mean confidence scores of 0.652 for true positives and 0.089 for true negatives, yielding a separation margin of 0.563.

Figure \ref{fig:confidence_analysis} illustrates the confidence distributions, showing that the model produces well-calibrated predictions with clear distinction between positive and negative instances. The optimal confidence threshold for deployment is determined to be 0.23, balancing precision and recall for practical legal document analysis.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.48\textwidth]{../figures/confidence_analysis.png}
\caption{Confidence analysis showing distribution by true labels and box plot comparison. True positive predictions show significantly higher confidence than true negatives.}
\label{fig:confidence_analysis}
\end{figure}

\subsection{Attention Pattern Analysis}

The attention visualization reveals that the model focuses on legally relevant terminology and phrases, demonstrating domain-appropriate behavior. Key findings include:

\begin{itemize}
    \item Strong attention to clause-specific keywords (e.g., "termination," "liability," "assignment")
    \item Appropriate focus on legal entities, dates, and contractual obligations
    \item Contextual understanding of complex legal language structures
\end{itemize}

\subsection{SHAP and LIME Interpretability}

Both SHAP and LIME explanations provide complementary insights into model predictions:

\begin{itemize}
    \item \textbf{SHAP Analysis}: Reveals global feature importance across all clause types, identifying the most influential legal terms and phrases
    \item \textbf{LIME Explanations}: Provides local interpretability for individual predictions, highlighting specific text segments that drive classification decisions
\end{itemize}

The explainability analysis confirms that the model's decision-making aligns with legal domain expertise, focusing on relevant contractual language and demonstrating interpretable reasoning patterns essential for legal practice adoption.

\subsection{Class Imbalance Impact}

The severe class imbalance (32.1% positive instances) significantly impacts model performance across different clause types. Rare clauses (appearing in <10% of documents) require specialized handling:

\begin{itemize}
    \item Lower confidence thresholds for rare clause detection
    \item Enhanced attention to context patterns for infrequent clause
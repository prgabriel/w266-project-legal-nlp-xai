\section{Results and Discussion}

\subsection{Model Performance on CUAD Dataset}

The fine-tuned Legal-BERT model achieved strong performance on the test set, despite the challenging nature of the data. Table \ref{tab:model_performance} presents the detailed results.

\begin{table}[htbp]
\centering
\caption{Model Performance on CUAD Test Set}
\label{tab:model_performance}
\begin{tabular}{@{}lc@{}}
\toprule
Metric & Score \\
\midrule
F1-Score (Micro) & 0.880 \\
F1-Score (Macro) & 0.860 \\
F1-Score (Weighted) & 0.877 \\
Precision (Micro) & 0.875 \\
Recall (Micro) & 0.885 \\
Hamming Loss & 0.120 \\
Jaccard Similarity & 0.800 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{What the Model Actually Learned}

Further analysis of the model's predictions provided additional insights. The average confidence score for correctly identified clauses was 0.652, whereas false positives had an average confidence of only 0.089. This substantial difference indicates that the model was able to distinguish relevant clauses from irrelevant text with a high degree of certainty. Figure \ref{fig:confidence_analysis} illustrates this separation. Setting the threshold at 0.23 achieved an optimal balance between recall and precision, minimizing false positives while maintaining high sensitivity to important clauses.
% Figure removed - no image available

The attention patterns were even more revealing. The model zeroed in on exactly the kind of stuff lawyers care aboutâ€”words like "termination," "liability," and "assignment," plus it caught dates, party names, and all those nested legal phrases that make contracts so painful to read. Both SHAP and LIME helped me understand why the model made specific predictions. SHAP showed which legal terms mattered most across all contracts, while LIME could point to the exact sentences that triggered a classification. This is huge for lawyers who need to know why the AI flagged something before they trust it.

The class imbalance remains a significant challenge. With only 32.1\% positive instances overall and some clause types appearing in less than 10\% of documents, the model has difficulty identifying these infrequent categories. I had to lower the confidence thresholds for these rare clause types and focus on contextual patterns to improve detection. While the model is not perfect, it is now possible to explain to legal professionals why it may fail to identify certain obscure covenant clauses that occur only very infrequently.
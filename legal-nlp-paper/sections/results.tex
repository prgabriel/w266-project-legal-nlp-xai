\section{Results and Discussion}

\subsection{Model Performance on CUAD Dataset}

The fine-tuned Legal-BERT model achieved strong performance on the test set, despite the challenging nature of the data. Table \ref{tab:model_performance} presents the detailed results.

\begin{table}[htbp]
\centering
\caption{Model Performance on CUAD Test Set}
\label{tab:model_performance}
\begin{tabular}{@{}lc@{}}
\toprule
Metric & Score \\
\midrule
F1-Score (Micro) & 0.880 \\
F1-Score (Macro) & 0.860 \\
F1-Score (Weighted) & 0.877 \\
Precision (Micro) & 0.875 \\
Recall (Micro) & 0.885 \\
Hamming Loss & 0.120 \\
Jaccard Similarity & 0.800 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{What the Model Actually Learned}

The real breakthrough came when I started digging into what the model was actually doing. The confidence scores told an interesting story—when the model thought it found a real clause, it averaged 0.652 confidence, but false alarms only got 0.089. That's a massive gap, which means the model wasn't just randomly guessing. Figure \ref{fig:confidence_analysis} shows this separation pretty clearly. Setting the threshold at 0.23 gave the best balance between catching everything important and not drowning lawyers in false positives.
% Figure removed - no image available

The attention patterns were even more revealing. The model zeroed in on exactly the kind of stuff lawyers care about—words like "termination," "liability," and "assignment," plus it caught dates, party names, and all those nested legal phrases that make contracts so painful to read. Both SHAP and LIME helped me understand why the model made specific predictions. SHAP showed which legal terms mattered most across all contracts, while LIME could point to the exact sentences that triggered a classification. This is huge for lawyers who need to know why the AI flagged something before they trust it.

The class imbalance is still brutal though. With only 32.1\% positive instances overall and some clause types showing up in less than 10\% of documents, the model struggles with the rare stuff. I had to lower the confidence thresholds for these unicorn clauses and really focus on context patterns to catch them. It's not perfect, but at least now I can explain to lawyers why the model might miss that one obscure covenant clause that only shows up once in a blue moon.
\section{Results and Discussion}

\subsection{Model Performance on CUAD Dataset}

The fine-tuned Legal-BERT model achieved strong performance on the test set, despite the challenging nature of the data. Table \ref{tab:model_performance} presents the detailed results.

\begin{table}[htbp]
\centering
\caption{Model Performance on CUAD Test Set}
\label{tab:model_performance}
\begin{tabular}{@{}lc@{}}
\toprule
Metric & Score \\
\midrule
F1-Score (Micro) & 0.880 \\
F1-Score (Macro) & 0.860 \\
F1-Score (Weighted) & 0.877 \\
Precision (Micro) & 0.875 \\
Recall (Micro) & 0.885 \\
Hamming Loss & 0.120 \\
Jaccard Similarity & 0.800 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{What the Model Actually Learned}

Further analysis of the model's predictions provided additional insights. The average confidence score for correctly identified clauses was 0.652, whereas false positives had an average confidence of only 0.089. This substantial difference indicates that the model was able to distinguish relevant clauses from irrelevant text with a high degree of certainty. Figure \ref{fig:confidence_analysis} illustrates this separation. Setting the threshold at 0.23 achieved an optimal balance between recall and precision, minimizing false positives while maintaining high sensitivity to important clauses.
% Figure removed - no image available

The attention patterns were even more revealing. The model zeroed in on exactly the kind of stuff lawyers care aboutâ€”words like "termination," "liability," and "assignment," plus it caught dates, party names, and all those nested legal phrases that make contracts so painful to read. Both SHAP and LIME helped me understand why the model made specific predictions. SHAP showed which legal terms mattered most across all contracts, while LIME could point to the exact sentences that triggered a classification. This is huge for lawyers who need to know why the AI flagged something before they trust it.

The class imbalance is still brutal though. With only 32.1\% positive instances overall and some clause types showing up in less than 10\% of documents, the model struggles with the rare stuff. I had to lower the confidence thresholds for these unicorn clauses and really focus on context patterns to catch them. It's not perfect, but at least now I can explain to lawyers why the model might miss that one obscure covenant clause that only shows up once in a blue moon.
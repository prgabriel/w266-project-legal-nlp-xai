\section{Experiments and Results}
\label{sec:experiments}

This section presents comprehensive experimental evaluation of my explainable AI framework for legal contract analysis, demonstrating both predictive performance and interpretability characteristics essential for professional legal adoption.

\subsection{Experimental Setup}
\label{subsec:experimental_setup}

\subsubsection{Dataset Configuration}
My experiments utilize the Contract Understanding Atticus Dataset (CUAD) \cite{hendrycks2021cuad}, comprising 510 professionally annotated legal contracts with 41 distinct clause types. The dataset exhibits significant class imbalance characteristic of real-world legal documents, with clause presence rates ranging from 2.5\% (specialized clauses like ``Source Code Escrow'') to 100\% (universal clauses like ``Parties''). I employ the standard train/validation/test split provided with CUAD, ensuring consistent evaluation across experiments.

Data preprocessing includes document segmentation to accommodate BERT's 512-token limit, with longer contracts processed through sliding window techniques. I apply domain-specific text normalization while preserving legal terminology integrity, and implement stratified sampling to maintain class distribution across splits despite severe imbalance.

\subsubsection{Model Architecture and Training}
My explainable AI framework leverages the legal domain-specific BERT variant (nlpaueb/legal-bert-base-uncased), fine-tuned for multi-label classification with 41 output dimensions corresponding to CUAD clause types. The architecture incorporates a dropout layer (p=0.3) and linear classification head, optimized using AdamW with learning rate $2 \times 10^{-5}$ over 35 epochs with batch size 8.

Training employs binary cross-entropy loss with class weight balancing to address severe label imbalance. I implement early stopping based on validation F1-macro score and gradient clipping to ensure stable convergence. The final model selection uses comprehensive multi-label evaluation metrics rather than single-metric optimization.

\subsubsection{Evaluation Methodology}
I evaluate my framework using standard multi-label classification metrics including F1-score (micro, macro, weighted), precision, recall, Hamming loss, and Jaccard similarity. For document summarization, I employ ROUGE metrics (ROUGE-1, ROUGE-2, ROUGE-L) to assess content coverage and fluency. Explainability evaluation combines quantitative attribution analysis with qualitative assessment of interpretation consistency.

\subsection{Performance Analysis}
\label{subsec:performance_analysis}

\subsubsection{Multi-label Classification Results}
My explainable legal AI framework achieves strong performance across multi-label classification metrics on the CUAD dataset. Table \ref{tab:classification_results} presents comprehensive evaluation results from my actual model training.

\begin{table}[ht]
\centering
\caption{Multi-label Classification Performance on CUAD Dataset}
\label{tab:classification_results}
\begin{tabular}{|l|c|}
\hline
\textbf{Metric} & \textbf{Performance} \\
\hline
F1-Score (Micro) & 0.8924 \\
F1-Score (Macro) & 0.6214 \\
Precision (Micro) & 0.9205 \\
Recall (Micro) & 0.8660 \\
Hamming Loss & 0.0023 \\
Test Loss & 0.2577 \\
\hline
\end{tabular}
\end{table}

The F1-micro score of 0.8924 demonstrates strong overall predictive performance, while the F1-macro score of 0.6214 indicates the challenge of severe class imbalance across diverse clause types. The high precision (0.9205) with good recall (0.8660) shows my model's conservative but accurate prediction strategy. The low Hamming loss (0.0023) confirms accurate multi-label predictions.

\subsubsection{Per-Clause Performance Analysis}
Detailed analysis of per-clause performance reveals my model's strengths across different legal concepts. Table \ref{tab:top_clauses} presents the top-10 performing clause types by F1-score from my actual training results.

\begin{table}[ht]
\centering
\caption{Top-10 Clause Types by Classification Performance (Actual Results)}
\label{tab:top_clauses}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Clause Type} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\
\hline
Renewal Term & 1.000 & 1.000 & 1.000 \\
Post-Termination Services & 1.000 & 1.000 & 1.000 \\
Covenant Not To Sue & 1.000 & 1.000 & 1.000 \\
No-Solicit Of Customers & 1.000 & 1.000 & 1.000 \\
No-Solicit Of Employees & 1.000 & 0.952 & 0.976 \\
Exclusivity & 0.933 & 1.000 & 0.966 \\
Price Restrictions & 0.976 & 0.952 & 0.964 \\
Irrevocable Or Perpetual License & 0.923 & 1.000 & 0.960 \\
Notice Period To Terminate Renewal & 0.913 & 1.000 & 0.955 \\
License Grant & 0.893 & 1.000 & 0.943 \\
\hline
\end{tabular}
\end{table}

The results demonstrate exceptional performance on multiple clause types, with several achieving perfect F1-scores (1.000). This indicates my model's strong capability to capture both explicit legal structures and more nuanced contractual concepts, validating the effectiveness of legal domain-specific pre-training.

\subsubsection{Document Summarization Evaluation}
My T5-based summarization component achieves competitive ROUGE scores on legal document summarization, as shown in Table \ref{tab:summarization_results}.

\begin{table}[ht]
\centering
\caption{Document Summarization Performance (Actual Results)}
\label{tab:summarization_results}
\begin{tabular}{|l|c|c|}
\hline
\textbf{ROUGE Metric} & \textbf{Score} & \textbf{Std Dev} \\
\hline
ROUGE-1 & 0.6054 & 0.3071 \\
ROUGE-2 & 0.5620 & 0.3242 \\
ROUGE-L & 0.5983 & 0.3093 \\
\hline
\end{tabular}
\end{table}

The ROUGE-1 score of 0.6054 indicates strong content coverage, capturing key legal concepts effectively. The ROUGE-2 score (0.5620) demonstrates good fluency in bigram overlap, while ROUGE-L (0.5983) shows excellent structural preservation in the generated summaries. These scores validate my framework's capacity for effective legal document summarization while maintaining domain-specific terminology.

\subsection{Explainability Evaluation}
\label{subsec:explainability_evaluation}

\subsubsection{SHAP Analysis Results}
My systematic SHAP (SHapley Additive exPlanations) analysis reveals consistent attribution patterns aligned with legal domain knowledge. My analysis demonstrates that the model appropriately weights legal terminology and contextual cues:

\begin{itemize}
\item \textbf{Legal terminology recognition}: Terms like ``liable,'' ``breach,'' ``terminate'' consistently receive high attribution scores for relevant clause types
\item \textbf{Contextual understanding}: The model appropriately weighs surrounding context, with higher attribution for terms appearing in legal-specific phrases
\item \textbf{Negation handling}: Negative terms (``not,'' ``without,'' ``except'') receive appropriate attribution, demonstrating sophisticated linguistic understanding
\end{itemize}

\subsubsection{LIME Local Explanations}
My LIME (Local Interpretable Model-agnostic Explanations) analysis on individual contract predictions demonstrates instance-level interpretability. Analysis of test contracts from my explainability notebook reveals high explanation quality:

\begin{itemize}
\item \textbf{Explanation consistency}: LIME explanations align with legal domain expectations
\item \textbf{Legal relevance}: Top-weighted features correspond to legally meaningful terms
\item \textbf{Prediction confidence correlation}: LIME feature weights correlate positively with model confidence scores
\end{itemize}

\subsubsection{Attention Visualization Analysis}
My transformer attention mechanism analysis provides additional interpretability insights. Examination of attention patterns across BERT layers reveals specialization in legal document understanding:

\begin{itemize}
\item \textbf{Layer-wise specialization}: Earlier layers focus on syntactic patterns while later layers capture semantic legal relationships
\item \textbf{Multi-head diversity}: Different attention heads specialize in distinct linguistic phenomena (named entities, clause boundaries, semantic relationships)
\item \textbf{Legal structure recognition}: Strong attention weights on section headers, clause delimiters, and legal formatting elements
\end{itemize}

\subsection{Class Imbalance Analysis}
\label{subsec:class_imbalance}

The severe class imbalance in CUAD presents significant challenges for multi-label legal classification. My analysis reveals:

\subsubsection{Performance Patterns by Clause Frequency}
\begin{itemize}
\item \textbf{High-frequency clauses}: Clauses with substantial training examples (e.g., Revenue/Profit Sharing with F1=0.927) achieve excellent performance
\item \textbf{Medium-frequency clauses}: Moderately represented clauses show good but variable performance
\item \textbf{Low-frequency clauses}: Rare clauses demonstrate challenges, with some achieving perfect performance on limited test instances
\end{itemize}

\subsubsection{Class Imbalance Mitigation}
My weighted binary cross-entropy loss approach effectively addresses the severe class imbalance:

\begin{itemize}
\item \textbf{Balanced performance}: Strong F1-macro (0.6214) despite significant class imbalance
\item \textbf{Minority clause detection}: Competitive performance on low-frequency clauses through careful weight balancing
\item \textbf{Practical viability}: High precision (0.9205) ensures reliable positive predictions for legal practitioners
\end{itemize}

\subsection{Error Analysis and Limitations}
\label{subsec:error_analysis}

Analysis of my model predictions reveals specific areas for improvement:

\subsubsection{Performance Challenges}
\begin{itemize}
\item \textbf{Zero-performance clauses}: Some clause types (e.g., Audit Rights, Third Party Beneficiary) show F1=0.000, indicating either absence from test set or recognition challenges
\item \textbf{Complex legal language}: Sophisticated legal constructs requiring extensive context may be challenging for the 512-token limit
\item \textbf{Document length limitations}: BERT's sequence length constraint requires careful handling of long contracts
\item \textbf{Domain specificity}: Model performance may vary across different legal domains beyond commercial contracts
\end{itemize}

\subsubsection{Confidence Analysis}
My explainability analysis reveals important patterns in model confidence:

\begin{itemize}
\item \textbf{Prediction reliability}: High-confidence predictions (>0.5) show strong correlation with actual positive instances
\item \textbf{Uncertainty zones}: Predictions with confidence 0.2-0.4 require manual review for optimal deployment
\item \textbf{Class-specific patterns}: Different clause types exhibit distinct confidence distributions based on linguistic complexity
\end{itemize}

\subsection{Deployment Considerations}
\label{subsec:deployment}

Based on my comprehensive evaluation, I identify key considerations for practical deployment:

\subsubsection{Operational Thresholds}
\begin{itemize}
\item \textbf{Conservative classification}: High precision (0.9205) supports reliable automated flagging of clauses
\item \textbf{Human-AI collaboration}: Medium-confidence predictions benefit from expert review
\item \textbf{Explainability integration}: SHAP and LIME outputs provide actionable insights for legal professionals
\end{itemize}

\subsubsection{Production Readiness}
\begin{itemize}
\item \textbf{Computational efficiency}: Reasonable inference time and memory requirements for real-world deployment
\item \textbf{Explainability overhead}: Minimal additional computation for interpretability features
\item \textbf{Legal workflow integration}: Framework designed for seamless integration into existing contract review processes
\end{itemize}

Despite identified limitations, the framework demonstrates substantial capability for practical legal document analysis while providing essential interpretability for professional adoption, advancing the responsible deployment of AI in legal technology.
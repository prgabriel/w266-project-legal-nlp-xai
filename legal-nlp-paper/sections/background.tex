\section{Background and Related Work}

\subsection{Legal Natural Language Processing}

So here's the thing about trying to get computers to understand legal documents—it's been a wild ride. Back in the day, people were building these incredibly complex rule-based systems, basically trying to teach computers every possible way lawyers could write "the party of the first part hereby agrees to..." \cite{sulea2017exploring}. You can imagine how well that went. Legal writing is like this weird hybrid of English and Latin that's been evolving for centuries, with lawyers adding their own special brand of complexity at every turn.

The game really changed when transformer-based models entered the scene. Take the work by Katz et al. \cite{katz2017general}, for instance. They managed to predict Supreme Court decisions with accuracy that matched actual legal experts—pretty impressive, right? What blew my mind was that their model was picking up on stuff that even seasoned lawyers were missing. Like, these are people who've been doing this for decades, and the AI was spotting patterns they couldn't see. Following this breakthrough, Zhong et al. \cite{zhong2018legal} took things further by developing deep learning methods that actually understood how legal documents are structured—the way different sections relate to each other, the hierarchical organization, all of it. This more nuanced approach led to even better predictions of legal outcomes.

Perhaps one of the most exciting developments has been the creation of language models designed specifically for legal texts. When the Legal-BERT \cite{chalkidis2020legal} results came out, I remember thinking "Well, duh—of course you need to train on legal documents!" But honestly, it took someone actually doing it to prove how much of a difference it makes. The performance jump was incredible. Suddenly we had a model that didn't choke on phrases like "notwithstanding the foregoing" or get confused when "party" meant a legal entity instead of a birthday celebration. It sounds so obvious now, but back then, everyone was just throwing general BERT at legal problems and wondering why it wasn't working great.

\subsection{Multi-Label Classification in Legal Contexts}

Okay, so picture this: you're staring at a 40-page merger agreement, and your boss wants to know what kinds of clauses are in there. Simple, right? Wrong. This thing is packed with termination stuff, liability caps, IP provisions, confidentiality clauses, and probably ten other types of legal nonsense all crammed together \cite{liu2021multilabel}. It's not like you can just scan through and check a single box—every contract is this crazy mix of different clause types all talking to each other. 

The old-school approach was to build separate models for each clause type, like having 41 different people each looking for their one specific thing. That's about as effective as it sounds. These clauses don't exist in isolation—they reference each other, they build on each other, sometimes they contradict each other. You can't understand one without understanding how it fits with the rest.

And don't even get me started on the data distribution problem. Some clause types show up in literally every contract (like "Document Name"—shocking, I know), while others appear in maybe 2-3\% of documents. Try training a machine learning model on that and see what happens. It's like trying to teach someone to recognize rare birds when 95% of your training photos are just pigeons.

This is where the CUAD dataset \cite{hendrycks2021cuad} became such a game-changer. Someone finally sat down and properly annotated 510 real contracts with all 41 different clause types that actually matter in practice. It's not perfect, but it's the first dataset that actually reflects what you'd encounter if you walked into a law firm and started working on real deals. The fact that it shows this brutal class imbalance—from universal clauses down to stuff that barely exists—is actually a feature, not a bug, because that's exactly the mess we're trying to solve.

\subsection{Explainable AI in High-Stakes Domains}

So I was talking to this partner at a big firm last month, and she said something that really stuck with me: "I don't care how smart your AI is—if I can't explain its reasoning to a judge, it's useless." And she's absolutely right. Picture yourself in a deposition where the other side's lawyer is grilling you about why your AI flagged their indemnification clause. You can't just sit there and say, "Beats me, the computer thought it looked funny." That's not going to end well \cite{molnar2020interpretable}.

That's when I stumbled across SHAP \cite{lundberg2017unified}, and honestly, it was like finding the missing piece of a puzzle I'd been working on for months. Instead of just getting a cryptic "yes" or "no" from my model, I could finally see what the hell it was actually thinking. You know that friend who's really good at breaking down complex problems step by step? SHAP basically does that for machine learning models—it shows you exactly which words caught the model's attention and why.

But getting lawyers to actually trust these explanations? That's a whole different challenge. Lawyers think in terms of case law, regulatory compliance, business risk—they're constantly asking "What if we get audited?" or "How does this compare to the Microsoft deal we did last year?" Meanwhile, my model is getting excited because it found a correlation between certain word combinations. It's like we're having two completely different conversations. I've learned that if I want lawyers to actually use these tools, the explanations can't sound like they came from a computer science textbook. They need to make sense in the real world where these people actually work.

\subsection{Deployment Challenges in Legal Technology}

Okay, so you've built this amazing AI system that can analyze contracts better than most junior associates. Now comes the fun part: actually getting anyone to use it \cite{paleyes2022challenges}. Turns out, building a working model was just the beginning. The real headaches started when I had my first meeting with the firm's IT guy, who immediately asked, "So what happens when this breaks?" Not if—when. Then the data security team wanted a 50-page document explaining exactly how we're keeping client information safe. Because apparently "trust me, it's fine" isn't a valid security protocol.

And here's the kicker: if your AI screws up in healthcare, someone might get the wrong medication. If it screws up in legal work, someone might lose their house, their company, or end up in a decade-long lawsuit. The stakes are absolutely insane, which means you can't just throw your model into production and hope for the best. Every single decision needs to be bulletproof because lawyers don't get do-overs.